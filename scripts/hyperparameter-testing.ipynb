{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5a18865",
   "metadata": {},
   "source": [
    "## Hyperparameter testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48ceea3",
   "metadata": {},
   "source": [
    "#### **verbs that we used in the study**\n",
    "- 4 ES, 4SE, 4 positive sentiment, 4 negative sentiment:\n",
    "    - faszinieren (\"fascinate\") (SE)\n",
    "    - inspirieren (\"inspire\") (SE)\n",
    "    - enttäuschen (\"disappoint\") (SE)\n",
    "    - schockieren (\"shock\") (SE)\n",
    "    - bewundern (\"admire\") (ES)\n",
    "    - respektieren (\"respect\") (ES)\n",
    "    - verabscheuen (\"despise\") (ES)\n",
    "    - hassen (\"hate\") (ES)\n",
    "\n",
    "\n",
    "#### **verbs that we use for hyperparameter testing**\n",
    "- 4 ES, 4SE, 4 positive sentiment, 4 negative sentiment\n",
    "    - stören (\"disturb\"/\"bother\") (SE)\n",
    "    - langeweilen (\"bore\") (SE)\n",
    "    - entzücken (\"delight\") (SE)\n",
    "    - amüsieren (\"amuse\") (SE)\n",
    "    - fürchten (\"fear\") (ES)\n",
    "    - beneiden (\"envy\") (ES)\n",
    "    - Mitleid mit DP haben (\"have pity with DP\") (ES)\n",
    "    - vergöttern  (\"idolize\"/\"adore\") (ES)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ea72f0",
   "metadata": {},
   "source": [
    "### Import packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01a1fafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "import pickle\n",
    "\n",
    "#Evaluation\n",
    "from datasets import load_metric\n",
    "from datasets import list_metrics\n",
    "import evaluate \n",
    "from evaluate import load"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ae2eac",
   "metadata": {},
   "source": [
    "### load and preprocess the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "563f5041",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/pkl_files/parameter_tests/gpt2_hptest_01.pkl\", \"rb\") as fp: \n",
    "    test01 = pickle.load(fp)\n",
    "    \n",
    "with open(\"../data/pkl_files/parameter_tests/gpt2_hptest_02.pkl\", \"rb\") as fp: \n",
    "    test02 = pickle.load(fp)\n",
    "       \n",
    "with open(\"../data/pkl_files/parameter_tests/gpt2_hptest_03.pkl\", \"rb\") as fp: \n",
    "    test03 = pickle.load(fp)   \n",
    "\n",
    "with open(\"../data/pkl_files/parameter_tests/gpt2_hptest_04.pkl\", \"rb\") as fp: \n",
    "    test04 = pickle.load(fp)\n",
    "    \n",
    "with open(\"../data/pkl_files/parameter_tests/gpt2_hptest_05.pkl\", \"rb\") as fp: \n",
    "    test05 = pickle.load(fp)\n",
    "\n",
    "with open(\"../data/pkl_files/parameter_tests/gpt2_hptest_06.pkl\", \"rb\") as fp: \n",
    "    test06 = pickle.load(fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e13a518a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## preprocess the data for automatic evaluation\n",
    "def create_lists_for_eval(datadict):\n",
    "    \"\"\"\n",
    "    Extracts the references and predictions from the datadict and returns them as a list,\n",
    "    so that they can be user for the automatic evaluation measures.\n",
    "    \n",
    "    Returns a list for references, diverse beam, nucleus sampling and typical sampling each\n",
    "    (in that order). \n",
    "    \"\"\"\n",
    "    \n",
    "    all_references =[]\n",
    "    all_divbeam_predictions = []\n",
    "    all_nucsamp_predictions = []\n",
    "    all_typsamp_predicitions = []\n",
    "    \n",
    "    for verb in datadict:\n",
    "        for i in range(0,(len(datadict[verb][\"prompt\"]))):\n",
    "            all_references.append(eval(datadict[verb][\"human_reference\"][i]))\n",
    "            # from the model generations, we still need to remove the prompts from the texts:\n",
    "            all_divbeam_predictions.append(\" \".join(datadict[verb][\"model_generation_diversebeam\"][i].partition(\",\")[2].split()[1:]))\n",
    "            all_nucsamp_predictions.append(\" \".join(datadict[verb][\"model_generation_nucleus_sampling\"][i].partition(\",\")[2].split()[1:]))\n",
    "            all_typsamp_predicitions.append(\" \".join(datadict[verb][\"model_generation_typical_sampling\"][i].partition(\",\")[2].split()[1:]))\n",
    "\n",
    "    return all_references, all_divbeam_predictions, all_nucsamp_predictions, all_typsamp_predicitions\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f84eca7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## call the function for each dictionary:\n",
    "\n",
    "test01_refs, test01_divbeam, test01_nucsamp, test01_typsamp = create_lists_for_eval(test01)\n",
    "\n",
    "test02_refs, test02_divbeam, test02_nucsamp, test02_typsamp = create_lists_for_eval(test02)\n",
    "\n",
    "test03_refs, test03_divbeam, test03_nucsamp, test03_typsamp = create_lists_for_eval(test03)\n",
    "\n",
    "test04_refs, test04_divbeam, test04_nucsamp, test04_typsamp = create_lists_for_eval(test04)\n",
    "\n",
    "test05_refs, test05_divbeam, test05_nucsamp, test05_typsamp = create_lists_for_eval(test05)\n",
    "\n",
    "test06_refs, test06_divbeam, test06_nucsamp, test06_typsamp = create_lists_for_eval(test06)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8ab91a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the references are always the same ones, so it is okay to use only one of these to compare with the different preds\n",
    "test01_refs == test02_refs == test03_refs == test04_refs == test05_refs == test06_refs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3b03fb",
   "metadata": {},
   "source": [
    "### BLEU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f64bb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu_metric = evaluate.load(\"bleu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a1f51960",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.39"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleuscore = bleu_metric.compute(references=test01_refs,\n",
    "                                predictions=test06_typsamp)\n",
    "\n",
    "round(bleuscore['precisions'][0],2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946e4922",
   "metadata": {},
   "source": [
    "### BERTScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "643848f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "bertscore_metric= load(\"bertscore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6da0b209",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.77"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bertscore = bertscore_metric.compute(references=test01_refs,\n",
    "                                     predictions=test06_typsamp, \n",
    "                                     lang=\"de\")\n",
    "\n",
    "round(bertscore['f1'][0],2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd133af",
   "metadata": {},
   "source": [
    "### GLEU (Google Bleu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1002c128",
   "metadata": {},
   "outputs": [],
   "source": [
    "gleu_metric = evaluate.load(\"google_bleu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "84b12b5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gleuscore = gleu_metric.compute(references=test01_refs,\n",
    "                                predictions=test06_typsamp)\n",
    "\n",
    "round(gleuscore[\"google_bleu\"],2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a560e600",
   "metadata": {},
   "source": [
    "### METEOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "60e1bc51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/judith/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/judith/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/judith/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "meteor_metric = evaluate.load(\"meteor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "587911f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meteorscore = meteor_metric.compute(references=test01_refs,\n",
    "                                    predictions=test06_typsamp)\n",
    "\n",
    "round(meteorscore[\"meteor\"],2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61713021",
   "metadata": {},
   "source": [
    "### ROUGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "aec5a51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_metric = evaluate.load('rouge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "14eb4ee6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.26"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rougescore = rouge_metric.compute(references=test01_refs,\n",
    "                                  predictions=test06_typsamp)\n",
    "\n",
    "round(rougescore[\"rougeL\"],2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
