{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "765b8859",
   "metadata": {},
   "source": [
    "## Produce continuations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173f111b",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28b1ab79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "import pickle \n",
    "\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc1c581",
   "metadata": {},
   "source": [
    "### Load and preprocess the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1532135e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all verbs used in the study: ['bewundern', 'enttäuschen', 'faszinieren', 'hassen ', 'inspirieren', 'respektieren', 'schockieren', 'verabscheuen']\n"
     ]
    }
   ],
   "source": [
    "## data used in the study:\n",
    "df = pd.read_excel(\"../data/prompts.xlsx\")\n",
    "\n",
    "## put all the verbs into a list\n",
    "verb_list = sorted(list(df[\"verb\"].value_counts().keys()))\n",
    "print(\"all verbs used in the study:\", sorted(verb_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf2c4a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all verbs used for parameter testing: ['Mitleid mit DP haben', 'amüsieren', 'beneiden', 'entzücken', 'fürchten', 'langeweilen ', 'stören', 'vergöttern']\n"
     ]
    }
   ],
   "source": [
    "## data used for hyperparameter testing:\n",
    "df_hptest = df = pd.read_excel(\"../data/prompts_for_hyperparameter_testing.xlsx\")\n",
    "\n",
    "## put all the verbs into a list\n",
    "verb_list_hptest = sorted(list(df_hptest[\"verb\"].value_counts().keys()))\n",
    "print(\"all verbs used for parameter testing:\", sorted(verb_list_hptest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ba402d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create a dictionary with all the data:\n",
    "\n",
    "def create_data_dict(dataset1, verblist):\n",
    "    \"\"\"\n",
    "    Takes the dataset and the verblist to create a (nested) dictionary with the (important) values.\n",
    "    The individual verbs are used as keys (of the outer dict).\n",
    "    \n",
    "    To be called for the individual models (GPT-2 etc.), to later fill \"model_generation\".\n",
    "    \"\"\"\n",
    "    data_dict = {}\n",
    "    for verb in verblist: \n",
    "        data_dict[verb] = {\"prompt_condition\": [],\n",
    "                           \"prompt\": [], \n",
    "                           \"arg_structure\": [],\n",
    "                           \"gender_order\": [], #order of male-female-arguments in the prompt\n",
    "                           \"sentiment\": [],\n",
    "                           \"human_reference\": [], #here, this is only the continuation (i.e. excluding the prompt)\n",
    "                           \"human_reference_selected\": [], #in case eval. metrics are used that can only handle one reference for one prediction\n",
    "                           \"generation_length\":[],\n",
    "                           # Decoding Procedures:\n",
    "                           \"model_generation_diversebeam\": [],\n",
    "                           \"model_generation_nucleus_sampling\": [],\n",
    "                           \"model_generation_typical_sampling\" : []\n",
    "                          } \n",
    "\n",
    "    for index, row in dataset1.iterrows():\n",
    "        for verb in verblist:\n",
    "            if row[\"verb\"] == verb: \n",
    "                data_dict[verb][\"prompt_condition\"].append(row[\"prompt condition\"])\n",
    "                data_dict[verb][\"prompt\"].append(row[\"prompt\"])\n",
    "                data_dict[verb][\"arg_structure\"].append(row[\"verb_type\"])\n",
    "                data_dict[verb][\"gender_order\"].append(row[\"gender-order\"])\n",
    "                data_dict[verb][\"sentiment\"].append(row[\"sentiment\"])\n",
    "                data_dict[verb][\"human_reference\"].append(row[\"human continuations\"]) \n",
    "    \n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6aa64e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## datadict which will contain the generations used in the study\n",
    "gendict = create_data_dict(dataset1=df, verblist=verb_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b78f2064",
   "metadata": {},
   "outputs": [],
   "source": [
    "## datadict which will contain the generations used for hyperparameter testing\n",
    "hptest_dict =  create_data_dict(dataset1=df_hptest, verblist=verb_list_hptest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d21062e",
   "metadata": {},
   "source": [
    "### Load models and generate continuations (for different decoding strategies)\n",
    "\n",
    "- Only do one at a time, save (with pickle below), then restart kernel (otherwise the generation of the different models will all be added into the same dictionary)\n",
    "\n",
    "- if dealing with a lot of data: move models (and tokenizers) to device "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e803116",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/opt/anaconda3/lib/python3.9/site-packages/transformers/models/auto/modeling_auto.py:1344: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "## Uncomment the one that should be used\n",
    "\n",
    "#### GPT-2: https://huggingface.co/dbmdz/german-gpt2\n",
    "from transformers import AutoTokenizer, AutoModelWithLMHead\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dbmdz/german-gpt2\")\n",
    "model = AutoModelWithLMHead.from_pretrained(\"dbmdz/german-gpt2\")#.to(\"mps\")\n",
    "\n",
    "#### mGPT: https://huggingface.co/ai-forever/mGPT \n",
    "#from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "#model = AutoModelForCausalLM.from_pretrained(\"sberbank-ai/mGPT\")\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"sberbank-ai/mGPT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2d3c143",
   "metadata": {},
   "outputs": [],
   "source": [
    "### create the prompt_list (needed as input for the models)\n",
    "def extract_prompts(datadict):\n",
    "    \"\"\"\n",
    "    Takes the datadict(i.e., all the data) as input. \n",
    "    Extracts the individual prompts and puts them into a list so that each prompt only appears once.\n",
    "    Returns this promptlist.\n",
    "    \"\"\"\n",
    "    prompt_list = [] \n",
    "    \n",
    "    for verb in datadict:\n",
    "        for prompt in list(datadict[verb][\"prompt\"]):\n",
    "            #print(prompt)\n",
    "            if prompt not in prompt_list:\n",
    "                prompt_list.append(prompt)\n",
    "            \n",
    "    return prompt_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb7fdc8",
   "metadata": {},
   "source": [
    "#### HYPERPARAMETERS\n",
    "\n",
    "- **results after hyperparameter testing with automatic evaluation metrics:**\n",
    "    - **diverse beam search:** best config (in 4/5 metrics): beam size =10 and diversity penalty = 0.7\n",
    "    - **nucleus sampling:** best config (in 3/5 metrics): top p = 0.85 with temperature = 0.7 \n",
    "    - **typical sampling:** best config (in 5/5 metrics): typical p = 0.9 with temperature = 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d2bc7c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x12ec218d0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GENERATION_LENGTH = 45\n",
    "# max. length of the model generation (but generations will be cut off after the first full stop)\n",
    "\n",
    "NO_REPEAT_N_GRAM_SIZE = 2  \n",
    "# -> n-gram penalties as introduced by Paulus et al. (2017) and Klein et al. (2017): \n",
    "# makes sure that no n-gram appears twice by manually setting the probability of next words that could create an already seen n-gram to 0.\n",
    "\n",
    "# For diverse beam search: \n",
    "BEAM_SIZE = 10\n",
    "DIVERSITY_PENALTY = 0.7\n",
    "\n",
    "# for nucleus (top-p) sampling \n",
    "P = 0.85\n",
    "\n",
    "#for (local) typical sampling \n",
    "TYPICAL_P = 0.9\n",
    "\n",
    "# for both sampling methods:\n",
    "TEMPERATURE = 0.7 \n",
    "\n",
    "# Set the seed for reproducibility\n",
    "torch.manual_seed(23)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf160ff",
   "metadata": {},
   "source": [
    "#### DECODING FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "12175e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diverse beam \n",
    "def generate_diversebeam(datadict, promptlist, model, tokenizer):\n",
    "    \"\"\"\n",
    "    Takes the datadict (i.e., all the data), the prompt list, the model and the tokenizer as input. \n",
    "    Generates the model continuations for the individual prompts from the list \n",
    "    and adds these continuations to the datadict so that every human reference has a respective model prediction. \n",
    "    Returns the dictionary with the added **diverse beam** model generations.\n",
    "    \n",
    "    *Beware: if the function has been called before (i.e., if there is already the respective data in the dict),\n",
    "    another function call will lead to duplicates! So only call once for every decoding strategy.*\n",
    "    \n",
    "    \"\"\"\n",
    "    generation_list=[]\n",
    "    # let model generate... \n",
    "    for prompt in promptlist:\n",
    "        # diverse beam \n",
    "        generation_list.append(tokenizer.decode(model.generate(**tokenizer(prompt, return_tensors=\"pt\"),#.to(\"mps\")), \n",
    "                                                              max_length=GENERATION_LENGTH,\n",
    "                                                              num_beams=BEAM_SIZE, \n",
    "                                                              num_beam_groups=BEAM_SIZE,\n",
    "                                                              #num_beam_groups must be <= num_beams                                     \n",
    "                                                              do_sample= False, \n",
    "                                                              no_repeat_ngram_size=NO_REPEAT_N_GRAM_SIZE, \n",
    "                                                              diversity_penalty= DIVERSITY_PENALTY, \n",
    "                                                              early_stopping=True)[0], skip_special_tokens=True))\n",
    "    \n",
    "    # now add these generations to the datadict\n",
    "    for verb in datadict:\n",
    "        for generation in generation_list:\n",
    "            for i in range(0, len(datadict[verb][\"prompt\"])):  \n",
    "                if datadict[verb][\"prompt\"][i] in generation:\n",
    "                    datadict[verb][\"model_generation_diversebeam\"].append(generation)\n",
    "                    datadict[verb][\"generation_length\"].append(GENERATION_LENGTH) \n",
    "                    \n",
    "    return datadict \n",
    "\n",
    "\n",
    "# Nucleus Sampling\n",
    "def generate_nucleus_sampling(datadict, promptlist, model, tokenizer):\n",
    "    \"\"\"\n",
    "    Does the same as generate_diverse_beam but only with the **nucleus-sampling (top-p)** decoding procedure!\n",
    "    \"\"\" \n",
    "    generation_list=[]\n",
    "    # let model generate... \n",
    "    for prompt in promptlist:\n",
    "        # Nucleus sampling/ top-p sampling: \n",
    "        generation_list.append(tokenizer.decode(model.generate(**tokenizer(prompt, return_tensors=\"pt\"),#.to(\"mps\")), \n",
    "                                                              max_length=GENERATION_LENGTH, \n",
    "                                                              do_sample=True, \n",
    "                                                              top_p=P,                                  \n",
    "                                                              top_k=0, #here k needs to be 0 \n",
    "                                                              temperature=TEMPERATURE, \n",
    "                                                              no_repeat_ngram_size=NO_REPEAT_N_GRAM_SIZE, \n",
    "                                                              early_stopping=True)[0], skip_special_tokens=True))\n",
    "    \n",
    "    # now add these generations to the datadict\n",
    "    for verb in datadict:\n",
    "        for generation in generation_list:\n",
    "            for i in range(0, len(datadict[verb][\"prompt\"])):  \n",
    "                if datadict[verb][\"prompt\"][i] in generation:\n",
    "                    datadict[verb][\"model_generation_nucleus_sampling\"].append(generation) \n",
    "                    \n",
    "    return datadict \n",
    "   \n",
    "    \n",
    "# (Local) Typical Sampling\n",
    "def generate_typical_sampling(datadict, promptlist, model, tokenizer):\n",
    "    \"\"\"\n",
    "    Does the same as generate_diverse_beam but only with the **typical sampling (typical p)** decoding procedure!\n",
    "    \"\"\" \n",
    "    generation_list=[]\n",
    "    # let model generate... \n",
    "    for prompt in promptlist:\n",
    "        # Nucleus sampling/ top-p sampling: \n",
    "        generation_list.append(tokenizer.decode(model.generate(**tokenizer(prompt, return_tensors=\"pt\"),#.to(\"mps\")), \n",
    "                                                              max_length=GENERATION_LENGTH, \n",
    "                                                              do_sample=True, \n",
    "                                                              typical_p=TYPICAL_P,                                  \n",
    "                                                              top_k=0,\n",
    "                                                              temperature=TEMPERATURE, \n",
    "                                                              no_repeat_ngram_size=NO_REPEAT_N_GRAM_SIZE, \n",
    "                                                              early_stopping=True)[0], skip_special_tokens=True))\n",
    "    \n",
    "    # now add these generations to the datadict\n",
    "    for verb in datadict:\n",
    "        for generation in generation_list:\n",
    "            for i in range(0, len(datadict[verb][\"prompt\"])):  \n",
    "                if datadict[verb][\"prompt\"][i] in generation:\n",
    "                    datadict[verb][\"model_generation_typical_sampling\"].append(generation) \n",
    "                    \n",
    "    return datadict \n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6434048",
   "metadata": {},
   "source": [
    "#### GENERATE...\n",
    "- call the function to produce the model generations and thereby add them into the data_dict\n",
    "- remember: only one model at a time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d8522476",
   "metadata": {},
   "outputs": [],
   "source": [
    "## hyperparamter testing:\n",
    "\n",
    "# first get the prompts\n",
    "#prompt_list_hptest = extract_prompts(datadict=hptest_dict)\n",
    "\n",
    "# call the decoding functions \n",
    "#hptest_dict = generate_diversebeam(datadict=hptest_dict, promptlist=prompt_list_hptest, \n",
    "#                                model =model, tokenizer=tokenizer)\n",
    "#hptest_dict = generate_nucleus_sampling(datadict=hptest_dict, promptlist=prompt_list_hptest, \n",
    "#                                     model =model, tokenizer=tokenizer)\n",
    "#hptest_dict = generate_typical_sampling(datadict=hptest_dict, promptlist=prompt_list_hptest, \n",
    "#                                     model =model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "60487d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## now with all the prompts for the study\n",
    "\n",
    "# first get the prompts\n",
    "prompt_list = extract_prompts(datadict=gendict)\n",
    "\n",
    "# call the decoding functions \n",
    "gendict = generate_diversebeam(datadict=gendict, promptlist=prompt_list, \n",
    "                                model =model, tokenizer=tokenizer)\n",
    "gendict = generate_nucleus_sampling(datadict=gendict, promptlist=prompt_list, \n",
    "                                     model =model, tokenizer=tokenizer)\n",
    "gendict = generate_typical_sampling(datadict=gendict, promptlist=prompt_list, \n",
    "                                     model =model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8bb6e3",
   "metadata": {},
   "source": [
    "#### Cut off the model generations after the first full stop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "abc79df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_sentence(sentence):\n",
    "    \"\"\"\n",
    "    Input = str (the sentence that needs to be modified).\n",
    "    Cuts off the sentence after the first full stop and returns this truncated sentence.\n",
    "    \"\"\"\n",
    "    # Find the index of the first full stop\n",
    "    full_stop_index = sentence.find('.')\n",
    "    # Cut off the sentence after the first full stop\n",
    "    cut_sentence = sentence[:full_stop_index+1]\n",
    "    \n",
    "    return cut_sentence\n",
    "\n",
    "## applying this to all generations of the datadict:\n",
    "def cut_generations(datadict):\n",
    "    \"\"\"\n",
    "    Input = dictionary (containing the model generations that needs to be cut off)\n",
    "    Applies the cut_sentence() function to all the model \n",
    "    \"\"\"\n",
    "    for verb in datadict:\n",
    "        for i in range(0, len(datadict[verb][\"prompt\"])):\n",
    "            datadict[verb][\"model_generation_diversebeam\"][i] = cut_sentence(datadict[verb][\"model_generation_diversebeam\"][i])\n",
    "            datadict[verb][\"model_generation_nucleus_sampling\"][i] = cut_sentence(datadict[verb][\"model_generation_nucleus_sampling\"][i])\n",
    "            datadict[verb][\"model_generation_typical_sampling\"][i] = cut_sentence(datadict[verb][\"model_generation_typical_sampling\"][i])\n",
    "        \n",
    "    return datadict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "29d1b93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hptest_dict = cut_generations(hptest_dict)\n",
    "\n",
    "gendict = cut_generations(gendict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "05aee313",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gendict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0827af3",
   "metadata": {},
   "source": [
    "#### Save the generations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5d9f0a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "### save the data dict\n",
    "with open (\"../data/pkl_files/gpt2.pkl\", \"wb\") as fp: \n",
    "    pickle.dump(gendict, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "88a02659",
   "metadata": {},
   "outputs": [],
   "source": [
    "### load data dict\n",
    "with open(\"../data/pkl_files/mgpt.pkl\", \"rb\") as fp: \n",
    "    mgpt_data = pickle.load(fp)\n",
    "\n",
    "with open(\"../data/pkl_files/gpt2.pkl\", \"rb\") as fp: \n",
    "    gpt2_data = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8572ad31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
